from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_ollama import ChatOllama
from langdetect import detect
import json
import logging
from typing import Literal
from .models import GraphState
from .rag_engine import RAGComponents
from .config import settings
import opencc
from .prompts import (
    CLASSIFICATION_PROMPT,
    CLARIFICATION_PROMPT,
    REWRITE_PROMPT_RETRY,
    REWRITE_PROMPT_NORMAL,
    GENERATE_SYSTEM_PROMPT,
    GUARDRAIL_PROMPT,
    OPTIMIZE_RESPONSE_PROMPT
)


logger = logging.getLogger(__name__)

class GraphBuilder:
    def __init__(self, rag_components):
        self.rag_engine = rag_components
        self.model = ChatOllama(
            model=settings.OLLAMA_MODEL, 
            base_url=settings.OLLAMA_BASE_URL, 
            temperature=0, 
            format="json"
        )
        
        # Classification Chain
        classification_prompt = ChatPromptTemplate.from_template(CLASSIFICATION_PROMPT)
        self.classification_chain = classification_prompt | self.model | StrOutputParser()
        
        # Clarification Chain
        clarification_prompt = ChatPromptTemplate.from_template(CLARIFICATION_PROMPT)
        # Clarification Chain
        clarification_prompt = ChatPromptTemplate.from_template(CLARIFICATION_PROMPT)
        self.clarification_chain = clarification_prompt | self.model | StrOutputParser()

        # Guardrail Chain (LLM-based)
        guardrail_prompt = ChatPromptTemplate.from_template(GUARDRAIL_PROMPT)
        self.guardrail_chain = guardrail_prompt | self.model | StrOutputParser()
        
        # Optimization Chain
        optimization_prompt = ChatPromptTemplate.from_template(OPTIMIZE_RESPONSE_PROMPT)
        self.optimization_chain = optimization_prompt | self.model | StrOutputParser()
        
        # Initialize OpenCC for Simplified to Traditional conversion
        self.cc = opencc.OpenCC('s2t')
    
    def _format_messages_to_str(self, messages) -> str:
        """Helper to format messages into a string history for rewriter/generator"""
        history_str = ""
        # æ’é™¤æœ€å¾Œä¸€æ¢ HumanMessageï¼Œå› ç‚ºå®ƒé€šå¸¸æ˜¯ç•¶å‰æ­£åœ¨è™•ç†çš„å•é¡Œ
        # é€™æ¨£å¯ä»¥é¿å… rewriter çœ‹åˆ°é‡è¤‡çš„å•é¡Œ
        msgs_to_process = messages[:-1] if messages and isinstance(messages[-1], HumanMessage) else messages
        
        for msg in msgs_to_process:
            if isinstance(msg, HumanMessage):
                history_str += f"Human: {msg.content}\n"
            elif isinstance(msg, SystemMessage):
                continue
            elif hasattr(msg, '__class__') and msg.__class__.__name__ == 'ToolMessage':
                # å°‡å·¥å…·çµæœæ‘˜è¦åŠ å…¥æ­·å²ï¼Œæœ‰åŠ©æ–¼ä¸Šä¸‹æ–‡ç†è§£
                content = str(msg.content)
                if len(content) > 100:
                    content = content[:100] + "..."
                history_str += f"System (Tool Result): {content}\n"
            else:
                # AI Message
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    # å¦‚æœ AI èª¿ç”¨äº†å·¥å…·ï¼Œè¨˜éŒ„ä¸€ä¸‹èª¿ç”¨çš„å‹•ä½œ
                    tool_names = [tc.get('name') for tc in msg.tool_calls]
                    history_str += f"AI (Action): èª¿ç”¨äº†å·¥å…· {', '.join(tool_names)}\n"
                
                if msg.content:
                    history_str += f"AI: {msg.content}\n"
    
        return history_str.strip()

    def initialize_conversation(self, state: GraphState) -> GraphState:
        """ç¯€é» 0: åˆå§‹åŒ–å°è©±ï¼Œå°‡ç”¨æˆ¶æ–°å•é¡ŒåŠ å…¥ messages"""
        logger.info("Checking conversation initialization...")
        original_query = state["original_query"]
        messages = state.get("messages", [])
        
        # æª¢æŸ¥æœ€å¾Œä¸€æ¢è¨Šæ¯æ˜¯å¦å·²ç¶“æ˜¯é€™å€‹å•é¡Œï¼ˆé¿å…é‡è¤‡æ·»åŠ ï¼‰
        if not messages or (isinstance(messages[-1], HumanMessage) and messages[-1].content != original_query) or not isinstance(messages[-1], HumanMessage):
             logger.info(f"Adding new user query to messages: {original_query[:50]}...")
             return {
                "messages": [HumanMessage(content=original_query)]
            }
        
        logger.info(f"Question already in messages, skipping duplication")
        return {"messages": []}

    def guardrail_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 0.5: è·¯ç”±å®ˆè¡›ï¼ˆLLM ç¯©é¸ï¼‰"""
        logger.info("Executing router guardrail (LLM)...")
        query = state["original_query"]
        messages = state.get("messages", [])
        
        # æº–å‚™å°è©±æ­·å²æ¦‚è¦
        history_str = self._format_messages_to_str(messages)
        
        # èª¿ç”¨ LLM åˆ¤æ–·
        try:
            response = self.guardrail_chain.invoke({
                "history_str": history_str if history_str else "ç„¡å…ˆå‰å°è©±",
                "query": query
            })
            result = json.loads(response)
            decision = result.get("decision", "allowed")
            reason = result.get("reason", "ç„¡åŸå› ")
            block_msg = result.get("response", "")
            
            logger.info(f"Guard decision: {decision} ({reason})")
            
            if decision == "blocked":
                logger.warning(f"Request blocked: {block_msg}")
                return {
                    "error": "blocked",
                    "final_answer": block_msg if block_msg else "æŠ±æ­‰ï¼Œæˆ‘åªèƒ½å›ç­”èˆ‡è«‹å‡æˆ–å·®å‹¤ç›¸é—œçš„å•é¡Œã€‚"
                }
            
        except Exception as e:
            logger.error(f"Guard execution error (defaulting to pass): {e}")
            return {"error": "pass"}
            
        logger.info("Request passed")
        return {"error": "pass"}
    
    def check_guardrail(self, state: GraphState) -> Literal["continue", "end"]:
        """æ¢ä»¶åˆ¤æ–·: å®ˆè¡›æ””æˆªçµæœ"""
        error = state.get("error")
        if error == "blocked":
            return "end"
        return "continue"

    def rewrite_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 1: æŸ¥è©¢é‡å¯«ï¼ˆæ”¯æ´å¤šè¼ªå°è©±ä¸Šä¸‹æ–‡ï¼‰"""
        logger.info("Executing query rewrite...")
        query = state["original_query"]
        retry_count = state.get("retry_count", 0)
        
        # å–å¾—å°è©±æ­·å²
        messages = state.get("messages", [])
        history_str = self._format_messages_to_str(messages)
        
        # ğŸ” èª¿è©¦è¼¸å‡ºï¼šæª¢æŸ¥æ˜¯å¦æœ‰æ­·å²
        logger.debug(f"History status - Messages: {len(messages)}, History len: {len(history_str)}")
        if history_str:
            logger.debug(f"History content: {history_str}")
        else:
            logger.debug("No history (likely first turn)")
        
        # æ ¹æ“šé‡è©¦æ¬¡æ•¸é¸æ“‡æç¤ºè©
        if retry_count > 0:
            logger.info(f"Retry count {retry_count}, attempting different keywords...")
            prompt = REWRITE_PROMPT_RETRY.format(
                history_str=history_str if history_str else "ç„¡å…ˆå‰å°è©±",
                query=query
            )
        else:
            prompt = REWRITE_PROMPT_NORMAL.format(
                history_str=history_str if history_str else "ç„¡å…ˆå‰å°è©±",
                query=query
            )
        
        rewritten = self.rag_engine.llm_rewriter.invoke(prompt).strip()
        
        logger.info(f"Original query: {query}")
        logger.info(f"Rewritten query: {rewritten}")
        logger.info(f"Used history: {'Yes' if history_str else 'No'}")
        
        return {
            "rewritten_query": rewritten,
            "retry_count": retry_count + 1
        }
    
    def classify_query(self, state: GraphState) -> GraphState:
        """ç¯€é» 2: æŸ¥è©¢åˆ†é¡"""
        logger.info("Executing query classification...")
        original_query = state["original_query"]
        query_to_classify = state.get("rewritten_query") or original_query
        
        response = self.classification_chain.invoke({"question": query_to_classify})
        
        try:
            category_data = json.loads(response)
            category = category_data.get("category", "other")
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse classification JSON: {response}")
            category = "other"
        
        logger.info(f"Original query: {original_query}")
        logger.info(f"Classification: {category}")
        
        return {"category": category}

    
    def retrieve_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 3: æ–‡ä»¶æª¢ç´¢"""
        logger.info("Executing document retrieval...")
        query = state["rewritten_query"]
        category = state.get("category", "other")
        
        retrieved_docs = self.rag_engine.search(query, category=category)
        
        return {"retrieved_docs": retrieved_docs}
    
    def rerank_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 4: æª¢ç´¢é‡æ’åº"""
        logger.info("Executing reranking...")
        query = state["rewritten_query"]
        docs = state.get("retrieved_docs", [])
        
        # é‡å°æª¢ç´¢åˆ°çš„å•é¡Œèˆ‡ä½¿ç”¨è€…å•é¡Œé€²è¡Œrerank
        reranked_docs = self.rag_engine.rerank(docs, query)
        
        # é™„åŠ ç­”æ¡ˆåˆ°æª¢ç´¢åˆ°çš„å•é¡Œ
        context_parts = []
        for i, doc in enumerate(reranked_docs, start=1):
            question = doc.page_content.replace('å•é¡Œ:', '').strip()
            answer = doc.metadata.get("answer", "")
            doc.page_content = f"å•é¡Œ: {question}\nç­”æ¡ˆ: {answer}"
            
            context_parts.append(
                f"ç¬¬{i}åç›¸é—œæ–‡ä»¶:\nå•é¡Œ: {question}\nç­”æ¡ˆ: {answer}"
            )
        
        context = "\n\n".join(context_parts)
        logger.debug(f"Context length: {len(context)}")
        
        return {
            "reranked_docs": reranked_docs,
            "context": context
        }
    
    def clarify_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 5: æª¢ç´¢çµæœé©—è­‰"""
        logger.info("Executing retrieval verification...")
        original_query = state["original_query"]
        context = state.get("context", "")
        
        if not context:
            logger.warning("No context found, defaulting to fail")
            return {"error": "no_content"}
            
        decision = self.clarification_chain.invoke({
            "question": original_query, 
            "context": context
        }).strip().lower()
        
        if "yes" in decision:
            decision = "yes"
        else:
            decision = "no"
        
        logger.info(f"Verification result: {decision}")
        
        return {"error": decision} 
    
    def generate_node(self, state: GraphState) -> GraphState:
        """ç¯€é» 4: ç­”æ¡ˆç”Ÿæˆ (æ”¯æ´ Tool Call)"""
        logger.info("Generating answer or calling tools...")
        
        messages = state.get("messages", [])
        logger.debug(f"Current messages count: {len(messages)}")
        
        # æ‰“å°è¨Šæ¯æ‘˜è¦
        for i, msg in enumerate(messages):
            msg_type = type(msg).__name__
            if isinstance(msg, HumanMessage):
                preview = f"Q: {msg.content[:50]}..."
            elif isinstance(msg, SystemMessage):
                preview = "[System]"
            elif hasattr(msg, 'tool_calls') and msg.tool_calls:
                preview = f"[Tool Call: {len(msg.tool_calls)}]"
            elif hasattr(msg, '__class__') and msg.__class__.__name__ == 'ToolMessage':
                preview = f"[Tool Result: {str(msg.content)[:50]}...]"
            else:
                preview = f"A: {str(msg.content)[:50]}..."
            logger.debug(f"  [{i}] {msg_type}: {preview}")
        
        # å–å¾—æ­·å²å°è©±å­—ä¸²ï¼ˆç”¨æ–¼æ§‹å»ºæ–°çš„ç³»çµ±æç¤ºï¼‰
        history_str = self._format_messages_to_str(messages)
        
        # å»ºç«‹ç³»çµ±æç¤ºè©
        system_prompt = GENERATE_SYSTEM_PROMPT.format(
            context=state.get('context')
        )
        
        # ğŸ”‘ æ§‹å»ºè¦ç™¼é€çµ¦ LLM çš„è¨Šæ¯åˆ—è¡¨
        # ç­–ç•¥ï¼šå§‹çµ‚åœ¨æœ€å‰é¢æ”¾ç½®æœ€æ–°çš„ SystemMessage
        llm_messages = [SystemMessage(content=system_prompt)]
        
        # ç„¶å¾ŒåŠ å…¥æ‰€æœ‰é SystemMessage çš„è¨Šæ¯
        for msg in messages:
            if not isinstance(msg, SystemMessage):
                llm_messages.append(msg)
        
        logger.debug(f"Calling LLM with {len(llm_messages)} messages")
        
        # å‘¼å« LLM
        response = self.rag_engine.llm_generator.invoke(llm_messages)
        
        logger.debug(f"LLM Response type: {type(response).__name__}")
        logger.debug(f"Content preview: {response.content[:150] if response.content else 'None'}...")
        if response.tool_calls:
            logger.info(f"Tool calls detected: {len(response.tool_calls)}")
            for tc in response.tool_calls:
                logger.debug(f"  - {tc.get('name', 'unknown')}")
        
        # åªè¿”å›æ–°çš„ responseï¼Œadd_messages æœƒè‡ªå‹•è¿½åŠ 
        return {
            "messages": [response],
            "final_answer": response.content if not response.tool_calls else None
        }


    def increment_tool_count(self, state: GraphState) -> GraphState:
        """åœ¨å·¥å…·åŸ·è¡Œå¾Œå¢åŠ è¨ˆæ•¸"""
        tool_call_count = state.get("tool_call_count", 0) + 1
        logger.info(f"Tool execution completed, count: {tool_call_count}")
        return {"tool_call_count": tool_call_count}
        
    def decide_to_rewrite(self, state: GraphState) -> Literal["rewrite", "generate"]:
        """æ¢ä»¶åˆ¤æ–·: æ˜¯å¦é‡å¯«"""
        decision = state.get("error")
        retry_count = state.get("retry_count", 0)
        
        if decision == "no" and retry_count < 3:
            logger.info("Retrieval validation failed, returning to Rewrite...")
            return "rewrite"
        else:
            if decision == "no":
                logger.warning("Max retries reached, proceeding to generation...")
            else:
                logger.info("Retrieval validation passed, proceeding to generation...")
            return "generate"
    
    def should_continue(self, state: GraphState) -> Literal["tools", "__end__"]:
        """æ¢ä»¶åˆ¤æ–·: æ˜¯å¦æ‡‰è©²ç¹¼çºŒï¼ˆèª¿ç”¨å·¥å…·æˆ–çµæŸï¼‰"""
        logger.debug("Deciding execution path (should_continue)...")
        
        messages = state.get("messages", [])
        tool_call_count = state.get("tool_call_count", 0)
        
        logger.debug(f"Current state - Messages: {len(messages)}, Tool count: {tool_call_count}")
        
        if not messages:
            logger.warning("Message list is empty, ending process")
            return END
        
        last_message = messages[-1]
        
        has_tool_calls = hasattr(last_message, 'tool_calls') and last_message.tool_calls
        
        if has_tool_calls:
            logger.debug(f"Tool calls detected: {last_message.tool_calls}")
            
            # æª¢æŸ¥å·¥å…·èª¿ç”¨æ¬¡æ•¸é™åˆ¶
            if tool_call_count >= 3:
                logger.warning("Max tool calls reached (3), forcing end")
                return END
            
            logger.info("Tool usage confirmed, switching to tools node")
            return "tools"
        else:
            logger.info("No tool calls, generation complete, proceeding to optimization")
            return "optimize"

    def optimize_response_node(self, state: GraphState) -> GraphState:
        """ç¯€é»: å›ç­”å„ªåŒ– (æ ¼å¼ã€èªè¨€ã€çµå°¾)"""
        logger.info("Executing response optimization...")
        
        final_answer = state.get("final_answer")
        
        # Fallback if final_answer is missing but last message is likely the answer
        if not final_answer:
            messages = state.get("messages", [])
            if messages and isinstance(messages[-1], AIMessage):
                final_answer = messages[-1].content
        
        if not final_answer:
             logger.warning("No answer to optimize.")
             return {}

        try:
            response = self.optimization_chain.invoke({"answer": final_answer})
            
            # Parse JSON response
            try:
                data = json.loads(response)
                # Get the optimized answer, or fallback to the whole response if key missing
                optimized_answer = data.get("optimized_answer") or data.get("response", response)
                
                # If for some reason the value is not a string (e.g. nested dict), convert to string
                if not isinstance(optimized_answer, str):
                    optimized_answer = str(optimized_answer)
                    
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse optimization JSON, using raw response: {response[:50]}...")
                optimized_answer = response

            # Ensure Traditional Chinese
            optimized_answer = self.cc.convert(optimized_answer)

            logger.info("Response optimized successfully.")
            return {"final_answer": optimized_answer}
        except Exception as e:
            logger.error(f"Optimization failed: {e}")
            return {"final_answer": final_answer + "\n\nè‹¥æœ‰å…¶ä»–éœ€æ±‚æ­¡è¿è©¢å•"}

    # æ­¤ç¯€é»æš«ä¸ä½¿ç”¨


    def build(self):
        """å»ºç«‹ LangGraph å·¥ä½œæµç¨‹"""
        workflow = StateGraph(GraphState)
        
        # æ·»åŠ æ‰€æœ‰ç¯€é»
        workflow.add_node("initialize", self.initialize_conversation)  # æ–°å¢
        workflow.add_node("guardrail", self.guardrail_node)      # æ–°å¢
        workflow.add_node("rewrite", self.rewrite_node)
        workflow.add_node("classify_query", self.classify_query)
        workflow.add_node("retrieve", self.retrieve_node)
        workflow.add_node("rerank", self.rerank_node) 
        workflow.add_node("clarify", self.clarify_node)
        workflow.add_node("generate", self.generate_node)
        workflow.add_node("tools", ToolNode(self.rag_engine.tools))
        workflow.add_node("increment_count", self.increment_tool_count)  # æ–°å¢
        workflow.add_node("optimize_response", self.optimize_response_node) # æ–°å¢å„ªåŒ–ç¯€é»

        # å®šç¾©æµç¨‹é‚Š
        workflow.set_entry_point("initialize")  # å¾åˆå§‹åŒ–é–‹å§‹
        workflow.add_edge("initialize", "guardrail") # æ”¹ç‚ºæ¥å®ˆè¡›
        
        # å®ˆè¡›æ¢ä»¶é‚Š
        workflow.add_conditional_edges(
            "guardrail",
            self.check_guardrail,
            {
                "continue": "rewrite",
                "end": END
            }
        )
        workflow.add_edge("rewrite", "classify_query")
        workflow.add_edge("classify_query", "retrieve")
        workflow.add_edge("retrieve", "rerank")
        workflow.add_edge("rerank", "clarify")
        
        # æ¢ä»¶é‚Šï¼šClarify -> Rewrite or Generate
        workflow.add_conditional_edges(
            "clarify",
            self.decide_to_rewrite,
            {
                "rewrite": "rewrite",
                "generate": "generate"
            }
        )

        # æ¢ä»¶é‚Šï¼šGenerate å¾Œæ±ºå®šæ˜¯å¦èª¿ç”¨å·¥å…·
        workflow.add_conditional_edges(
            "generate",
            self.should_continue,
            {
                "tools": "tools",
                "optimize": "optimize_response"
            }
        )

        # å„ªåŒ–å®Œæˆå¾ŒçµæŸ
        workflow.add_edge("optimize_response", END)

        # å·¥å…·åŸ·è¡Œå®Œ -> å¢åŠ è¨ˆæ•¸ -> å›åˆ° generate
        workflow.add_edge("tools", "increment_count")
        workflow.add_edge("increment_count", "generate")

        checkpointer = MemorySaver()
        return workflow.compile(checkpointer=checkpointer)